{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object oriented programming (OOP): Reinforcement Learning\n",
    "\n",
    "Example from: https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global variables\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (0, 3)\n",
    "LOSE_STATE = (1, 3)\n",
    "START = (2, 0)\n",
    "DETERMINISTIC = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule for the agent\n",
    "\n",
    "Your agent starts at the left-bottom corner(the 'start' sign) and ends at either +1 or -1 which is the corresponding reward. At each step, the agent has 4 possible actions including up, down, left and right, whereas the black cell is a wall where your agent wonâ€™t be able to penetrate through. In order to make it more straight forward, our first implementation assumes that each action is deterministic, that is, the agent will go where it intends to go. For instance, when the agent decides to take action up at (2, 0), it will land in (1, 0) rather than (2, 1) or elsewhere. (We will add uncertainty in out second implementation) However, it the agents hit the wall, it will remain at the same position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the board with zeros\n",
    "board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "\n",
    "# Mark the start, win, and lose states on the grid\n",
    "board[START] = 0.5\n",
    "board[WIN_STATE] = 1\n",
    "board[LOSE_STATE] = -1\n",
    "\n",
    "# Block cell\n",
    "block_state = (1, 1)\n",
    "board[block_state] = np.nan  # Use NaN for blocked cells\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Use a heatmap to show the different states\n",
    "cmap = plt.cm.coolwarm\n",
    "cmap.set_bad('black', 1.)\n",
    "heatmap = ax.imshow(board, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "# Add text in each cell\n",
    "for i in range(BOARD_ROWS):\n",
    "    for j in range(BOARD_COLS):\n",
    "        if (i, j) == WIN_STATE:\n",
    "            text = ax.text(j, i, '+1', ha='center', va='center', color='white')\n",
    "        elif (i, j) == LOSE_STATE:\n",
    "            text = ax.text(j, i, '-1', ha='center', va='center', color='white')\n",
    "        elif (i, j) == START:\n",
    "            text = ax.text(j, i, 'start', ha='center', va='center', color='white')\n",
    "\n",
    "# Set the grid\n",
    "ax.set_xticks(np.arange(-.5, BOARD_COLS, 1), minor=False)\n",
    "ax.set_yticks(np.arange(-.5, BOARD_ROWS, 1), minor=False)\n",
    "ax.grid(color=\"black\", linestyle='-', linewidth=0.5)\n",
    "ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "# Remove the tick labels\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Agent Classes with their methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC\n",
    "\n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxtState = (self.state[0] - 1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nxtState = (self.state[0] + 1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nxtState = (self.state[0], self.state[1] - 1)\n",
    "            else:\n",
    "                nxtState = (self.state[0], self.state[1] + 1)\n",
    "            # if next state legal\n",
    "            if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS -1)):\n",
    "                if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS -1)):\n",
    "                    if nxtState != (1, 1):\n",
    "                        return nxtState\n",
    "            return self.state\n",
    "\n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "\n",
    "# Agent\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = State()\n",
    "        self.lr = 0.2 # learning rate\n",
    "        self.exp_rate = 0.3 # exploration rate\n",
    "\n",
    "        # initial state reward\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = 0  # set initial value to 0\n",
    "\n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                # if the action is deterministic\n",
    "                nxt_reward = self.state_values[self.State.nxtPosition(a)]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "        return action\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        return State(state=position)\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.State = State()\n",
    "\n",
    "    def play(self, rounds=10):\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            # to the end of game back propagate reward\n",
    "            if self.State.isEnd:\n",
    "                # back propagate\n",
    "                reward = self.State.giveReward()\n",
    "                # explicitly assign end state to reward values\n",
    "                self.state_values[self.State.state] = reward  # this is optional\n",
    "                print(\"Game End Reward\", reward)\n",
    "                for s in reversed(self.states):\n",
    "                    reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n",
    "                    self.state_values[s] = round(reward, 3)\n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.chooseAction()\n",
    "                # append trace\n",
    "                self.states.append(self.State.nxtPosition(action))\n",
    "                print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.State = self.takeAction(action)\n",
    "                # mark is end\n",
    "                self.State.isEndFunc()\n",
    "                print(\"nxt state\", self.State.state)\n",
    "                # print(\"---------------------\")\n",
    "\n",
    "    def showValues(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('----------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class instance and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = Agent()\n",
    "ag.play(100)\n",
    "\n",
    "print(ag.showValues())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three lines in the result table represent the state values for a 3-row grid environment in the agent-based model. Each row corresponds to a different horizontal level in the grid, and each cell within a row represents a specific position or state in that level. The grid layout and the values illustrate how the agent has learned to value each position based on its experiences of rewards and penalties throughout its exploration of the environment.<br>\n",
    "\n",
    "<div style=\"color: blue;\">Example output:</div><br>\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <td>0.933</td>\n",
    "    <td>0.935</td>\n",
    "    <td>0.920</td>\n",
    "    <td>1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.931</td>\n",
    "    <td>0</td>\n",
    "    <td>0.729</td>\n",
    "    <td>-1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.929</td>\n",
    "    <td>0.927</td>\n",
    "    <td>0.911</td>\n",
    "    <td>0.662</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
